# Distributed Representations of Words and Phrases and their Compositionality
Transformers paper presentation

## Overview

### The problem the paper is addressing:

How to train distributed representations of words and phrases with Skip-gram model.

### Characterise the approach:

Skip-gram models computes the probability for each word of appearing in the context independently of its distance to the center word.


<img width="968" alt="Screen Shot 2022-03-28 at 12 30 18 PM" src="https://user-images.githubusercontent.com/65924566/160454237-4d3afb9e-10b5-4efd-9adb-8ee58ccacd5e.png">

## The diagram of Skip-gram architecture:

<img width="1103" alt="Screen Shot 2022-03-28 at 12 31 49 PM" src="https://user-images.githubusercontent.com/65924566/160454469-e44ab701-7cff-4a35-8502-f28d116560ce.png">

## Discussion Topic 1:

<img width="999" alt="Screen Shot 2022-03-28 at 12 33 52 PM" src="https://user-images.githubusercontent.com/65924566/160454797-11e92c4f-6e29-4e7c-80eb-e53b97bf4be1.png">

## Discussion Topic 2:

<img width="1011" alt="Screen Shot 2022-03-28 at 12 34 49 PM" src="https://user-images.githubusercontent.com/65924566/160454936-2a091962-dcc9-44dd-adb3-5bda86defdd6.png">

<img width="1051" alt="Screen Shot 2022-03-28 at 12 35 05 PM" src="https://user-images.githubusercontent.com/65924566/160454969-61f91d82-3f9e-437a-aec4-8b4f85addb29.png">

## Discussion Topic 3:

<img width="1010" alt="Screen Shot 2022-03-28 at 12 37 12 PM" src="https://user-images.githubusercontent.com/65924566/160455270-aaccc1f0-4c72-4c14-9d9f-91dde66755ab.png">

<img width="1115" alt="Screen Shot 2022-03-28 at 12 36 57 PM" src="https://user-images.githubusercontent.com/65924566/160455241-7117353c-f6ad-4c61-a4c6-1be4b377affd.png">

## Critical Analysis:

<img width="1055" alt="Screen Shot 2022-03-28 at 9 56 35 PM" src="https://user-images.githubusercontent.com/65924566/160523906-2a6bc4f2-37d5-4b62-b0d6-ab22e38d69ce.png">

## Resource Link:

1. Original Paper link: https://arxiv.org/pdf/1310.4546.pdf 
2. Efficient Estimation of Word Representations in Vector Space: https://arxiv.org/abs/1301.3781 
3. GloVe: Global Vectors for Word Representation: https://nlp.stanford.edu/pubs/glove.pdf 
4. A Latent Variable Model Approach to PMI-based Word Embeddings: https://arxiv.org/abs/1502.03520 
5. word2vec Parameter Learning Explained:            https://arxiv.org/abs/1411.2738 
6. wevi: word embedding visual inspector:              https://ronxin.github.io/wevi/ 

